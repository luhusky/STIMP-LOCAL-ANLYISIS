import warnings

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

import torch
import os
import platform
from torch.utils.data import DataLoader
import logging
import time
from tqdm import tqdm
from timm.utils import AverageMeter
import numpy as np
import argparse
import sys

# æ£€æŸ¥æ“ä½œç³»ç»Ÿ
IS_WINDOWS = platform.system() == "Windows"
if IS_WINDOWS:
    print("ğŸ”§ æ£€æµ‹åˆ° Windows ç³»ç»Ÿï¼Œè¿›è¡Œå…¼å®¹æ€§è°ƒæ•´")

# å¯¼å…¥ SwanLab
try:
    import swanlab

    SWANLAB_AVAILABLE = True
except ImportError:
    print(" SwanLab æœªå®‰è£…ï¼Œä½¿ç”¨ pip install swanlab å®‰è£…ä»¥è·å¾—æ›´å¥½çš„å®éªŒè·Ÿè¸ª")
    SWANLAB_AVAILABLE = False

sys.path.insert(0, os.getcwd())
from dataset.dataset_imputation import PRE8dDataset
from utils import check_dir, masked_mae, masked_mse, seed_everything


class EarlyStopping:
    """æ—©åœæ­¢ç±»ï¼Œç”¨äºåœ¨éªŒè¯æŸå¤±ä¸å†æ”¹å–„æ—¶åœæ­¢è®­ç»ƒ"""

    def __init__(self, patience=10, verbose=True, delta=0, path='checkpoint.pt'):
        """
        Args:
            patience (int): éªŒè¯æŸå¤±ä¸å†æ”¹å–„çš„ç­‰å¾…è½®æ•°
            verbose (bool): æ˜¯å¦æ‰“å°æ—©åœæ­¢ä¿¡æ¯
            delta (float): è®¤ä¸ºæœ‰æ”¹å–„çš„æœ€å°å˜åŒ–é‡
            path (str): æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta
        self.path = path

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'ğŸš¨ æ—©åœæ­¢è®¡æ•°å™¨: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''ä¿å­˜æ¨¡å‹å½“éªŒè¯æŸå¤±å‡å°‘æ—¶'''
        if self.verbose:
            print(f'ğŸ¯ éªŒè¯æŸå¤±å‡å°‘ ({self.val_loss_min:.6f} --> {val_loss:.6f}). ä¿å­˜æ¨¡å‹...')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss


class TrainConfig:
    def __init__(self):
        parser = argparse.ArgumentParser(description='STIMPæ’è¡¥è®­ç»ƒï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰')
        # åŒºåŸŸå’Œæ•°æ®è·¯å¾„ - ä½¿ç”¨ä¿®å¤åçš„æ•°æ®è·¯å¾„
        parser.add_argument('--area', type=str, default='Bohai', help='ç›®æ ‡åŒºåŸŸï¼ˆå¦‚Bohaiï¼‰')
        parser.add_argument('--raw_data_path', type=str, default=r'E:\1workinANHUA\4\data\Himawari-bohaidata-fixed',
                            help='ä¿®å¤åçš„åŸå§‹æ•°æ®è·¯å¾„')
        # è®­ç»ƒå‚æ•°
        parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
        parser.add_argument('--batch_size', type=int, default=1, help='æ‰¹æ¬¡å¤§å°')
        parser.add_argument('--lr', type=float, default=1e-3, help='å­¦ä¹ ç‡')
        parser.add_argument('--wd', type=float, default=1e-4, help='æƒé‡è¡°å‡')
        parser.add_argument('--test_freq', type=int, default=25, help='æµ‹è¯•é¢‘ç‡')
        # æ—©åœæ­¢å‚æ•°
        parser.add_argument('--early_stopping_patience', type=int, default=10, help='æ—©åœæ­¢è€å¿ƒå€¼')
        # æ¨¡å‹å‚æ•°
        parser.add_argument('--embedding_size', type=int, default=4, help='åµŒå…¥ç»´åº¦')
        parser.add_argument('--hidden_channels', type=int, default=4, help='éšè—å±‚ç»´åº¦')
        parser.add_argument('--diffusion_embedding_size', type=int, default=64, help='æ‰©æ•£åµŒå…¥ç»´åº¦')
        parser.add_argument('--side_channels', type=int, default=1, help='è¾…åŠ©é€šé“æ•°')
        # ä»»åŠ¡å‚æ•°
        parser.add_argument('--in_len', type=int, default=24, help='è¾“å…¥åºåˆ—é•¿åº¦')
        parser.add_argument('--out_len', type=int, default=24, help='è¾“å‡ºåºåˆ—é•¿åº¦')
        parser.add_argument('--missing_ratio', type=float, default=0.1, help='ç¼ºå¤±ç‡')
        # æ‰©æ•£å‚æ•°
        parser.add_argument('--beta_start', type=float, default=0.0001, help='betaèµ·å§‹å€¼')
        parser.add_argument('--beta_end', type=float, default=0.2, help='betaç»“æŸå€¼')
        parser.add_argument('--num_steps', type=int, default=50, help='æ‰©æ•£æ­¥æ•°')
        parser.add_argument('--num_samples', type=int, default=1, help='æ’è¡¥æ ·æœ¬æ•°')
        parser.add_argument('--schedule', type=str, default='quad', help='æ‰©æ•£è°ƒåº¦')
        parser.add_argument('--target_strategy', type=str, default='random', help='æ©ç ç­–ç•¥')
        # æ³¨æ„åŠ›å‚æ•°
        parser.add_argument('--num_heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
        # ç”Ÿæˆæ’è¡¥æ•°æ®å‚æ•°
        parser.add_argument('--generate_imputation', action='store_true', help='è®­ç»ƒåç”Ÿæˆæ’è¡¥æ•°æ®')
        parser.add_argument('--num_generate_samples', type=int, default=10, help='ç”Ÿæˆæ ·æœ¬æ•°')
        # SwanLab é…ç½®
        parser.add_argument('--swanlab_project', type=str, default='STIMP-Bohai-Fixed', help='SwanLabé¡¹ç›®åç§°')
        parser.add_argument('--swanlab_experiment', type=str, default=None, help='SwanLabå®éªŒåç§°')

        self.args = parser.parse_args()

        # è®¾ç½®å®éªŒåç§°
        if self.args.swanlab_experiment is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
            self.args.swanlab_experiment = f"{self.args.area}_missing_{self.args.missing_ratio}_{timestamp}"

        # è®¾ç½®ç©ºé—´ç»´åº¦
        if self.args.area == "MEXICO":
            self.args.height, self.args.width = 36, 120
        elif self.args.area == "PRE":
            self.args.height, self.args.width = 60, 96
        elif self.args.area == "Chesapeake":
            self.args.height, self.args.width = 60, 48
        elif self.args.area == "Yangtze":
            self.args.height, self.args.width = 96, 72
        elif self.args.area in ["Himawari", "Bohai"]:
            self.args.height, self.args.width = 128, 128
        else:
            raise ValueError(f"æœªæ”¯æŒçš„åŒºåŸŸ: {self.args.area}")

    def __getattr__(self, name):
        return getattr(self.args, name)


def setup_swanlab(config):
    """è®¾ç½® SwanLab å®éªŒè·Ÿè¸ª"""
    if not SWANLAB_AVAILABLE:
        return None

    # SwanLab é…ç½®
    swanlab_config = {
        # è®­ç»ƒå‚æ•°
        "area": config.area,
        "epochs": config.epochs,
        "batch_size": config.batch_size,
        "learning_rate": config.lr,
        "weight_decay": config.wd,
        "test_frequency": config.test_freq,
        "early_stopping_patience": config.early_stopping_patience,
        # æ¨¡å‹å‚æ•°
        "embedding_size": config.embedding_size,
        "hidden_channels": config.hidden_channels,
        "diffusion_embedding_size": config.diffusion_embedding_size,
        # ä»»åŠ¡å‚æ•°
        "input_length": config.in_len,
        "output_length": config.out_len,
        "missing_ratio": config.missing_ratio,
        # æ‰©æ•£å‚æ•°
        "beta_start": config.beta_start,
        "beta_end": config.beta_end,
        "num_steps": config.num_steps,
        "num_samples": config.num_samples,
        "schedule": config.schedule,
    }

    try:
        # åˆå§‹åŒ– SwanLab
        run = swanlab.init(
            project=config.swanlab_project,
            experiment_name=config.swanlab_experiment,
            config=swanlab_config,
            description=f"STIMP model training with FIXED data for {config.area} with missing ratio {config.missing_ratio}"
        )

        print(f"ğŸ”¬ SwanLab å®éªŒè·Ÿè¸ªå·²å¯åŠ¨")
        return run
    except Exception as e:
        print(f"âš ï¸ SwanLab åˆå§‹åŒ–å¤±è´¥: {e}")
        print("å°†ç»§ç»­è®­ç»ƒï¼Œä½†ä¸è¿›è¡Œå®éªŒè·Ÿè¸ª")
        return None


def calculate_metrics_fixed(imputed, original, mask):
    """è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡ - ä¿®å¤ç‰ˆæœ¬"""
    imputed_flat = imputed.cpu().squeeze()
    original_flat = original.cpu().squeeze()
    mask_flat = mask.cpu().squeeze()

    # è°ƒè¯•ä¿¡æ¯
    print(f"è°ƒè¯•ä¿¡æ¯ - è¾“å…¥å½¢çŠ¶: imputed{imputed_flat.shape}, original{original_flat.shape}, mask{mask_flat.shape}")
    print(f"è°ƒè¯•ä¿¡æ¯ - æ•°æ®èŒƒå›´: imputed[{imputed_flat.min():.4f}, {imputed_flat.max():.4f}], "
          f"original[{original_flat.min():.4f}, {original_flat.max():.4f}]")

    # åªè®¡ç®—ç¼ºå¤±ä½ç½® (mask=0è¡¨ç¤ºç¼ºå¤±ï¼Œ1è¡¨ç¤ºè§‚æµ‹)
    valid_mask = (1 - mask_flat) > 0.5



    if valid_mask.sum() == 0:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°éœ€è¦æ’è¡¥çš„ç¼ºå¤±ä½ç½®")
        # å¦‚æœæ²¡æœ‰ç¼ºå¤±ä½ç½®ï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„æŒ‡æ ‡ä½œä¸ºå‚è€ƒ
        valid_mask = torch.ones_like(mask_flat).bool()

    mae = masked_mae(imputed_flat, original_flat, valid_mask)
    mse = masked_mse(imputed_flat, original_flat, valid_mask)
    rmse = torch.sqrt(mse)

    print(f"è°ƒè¯•ä¿¡æ¯ - æœ‰æ•ˆä½ç½®æ•°: {valid_mask.sum()}, MAE: {mae:.4f}")

    return mae, mse, rmse


def validate_data_before_training(config):
    """åœ¨è®­ç»ƒå‰éªŒè¯æ•°æ®è´¨é‡"""
    print("ğŸ” è®­ç»ƒå‰æ•°æ®éªŒè¯...")

    try:
        # åŠ è½½æ•°æ®é›†è¿›è¡ŒéªŒè¯
        os.environ['RAW_DATA_PATH'] = config.raw_data_path
        train_dataset = PRE8dDataset(config.args, mode='train')

        # æ£€æŸ¥ä¸€ä¸ªæ ·æœ¬
        sample = train_dataset[0]
        input_seq, input_ob_mask, input_gt_mask, output_seq, output_ob_mask = sample

        print("æ ·æœ¬æ•°æ®æ£€æŸ¥:")
        print(f"  è¾“å…¥åºåˆ—å½¢çŠ¶: {input_seq.shape}")
        print(f"  è¾“å…¥åºåˆ—èŒƒå›´: [{input_seq.min():.4f}, {input_seq.max():.4f}]")
        print(f"  è§‚æµ‹æ©ç ä¸­1çš„æ¯”ä¾‹: {input_ob_mask.mean():.4f}")
        print(f"  çœŸå®æ©ç ä¸­1çš„æ¯”ä¾‹: {input_gt_mask.mean():.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        if input_seq.max() > 100 or input_seq.min() < -100:
            print("âŒ è­¦å‘Š: è¾“å…¥æ•°æ®èŒƒå›´å¼‚å¸¸ï¼Œå¯èƒ½å­˜åœ¨å½’ä¸€åŒ–é—®é¢˜")
            return False
        else:
            print("âœ… æ•°æ®èŒƒå›´æ­£å¸¸")
            return True

    except Exception as e:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return False


def generate_imputation_data(config, model, test_loader, adj, device):
    """ç”Ÿæˆæ’è¡¥æ•°æ®"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆæ’è¡¥æ•°æ®...")
    model.eval()

    all_imputed_data = []
    all_original_data = []
    all_masks = []
    all_input_ob_masks = []

    with torch.no_grad():
        for step, (datas, data_ob_masks, data_gt_masks, labels, label_masks) in enumerate(tqdm(test_loader)):
            # æ•°æ®ç§»è‡³è®¾å¤‡
            datas = datas.float().to(device)
            data_ob_masks = data_ob_masks.float().to(device)
            data_gt_masks = data_gt_masks.float().to(device)
            adj_batch = adj.repeat(datas.shape[0], 1, 1).to(device)

            # ç”Ÿæˆæ’è¡¥æ•°æ®
            imputed = model.impute(datas, data_gt_masks, adj_batch, config.num_generate_samples)

            # å–ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆç»“æœ
            imputed_median = imputed.median(dim=1).values

            # ä¿å­˜ç»“æœ
            all_imputed_data.append(imputed_median.cpu().numpy())
            all_original_data.append(datas.cpu().numpy())
            all_masks.append(data_gt_masks.cpu().numpy())
            all_input_ob_masks.append(data_ob_masks.cpu().numpy())

            # é™åˆ¶å¤„ç†æ‰¹æ¬¡æ•°é‡ï¼ˆå¯é€‰ï¼‰
            if step >= 50:  # å¤„ç†å‰50ä¸ªæ‰¹æ¬¡
                break

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    imputed_data = np.concatenate(all_imputed_data, axis=0)
    original_data = np.concatenate(all_original_data, axis=0)
    masks = np.concatenate(all_masks, axis=0)
    input_ob_masks = np.concatenate(all_input_ob_masks, axis=0)

    # ä¿å­˜ç»“æœ
    output_dir = f"./imputation_results/{config.area}"
    os.makedirs(output_dir, exist_ok=True)

    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    output_path = os.path.join(
        output_dir,
        f"stimp_imputation_{config.area}_missing_{config.missing_ratio}_{timestamp}.npz"
    )

    np.savez_compressed(
        output_path,
        imputed_data=imputed_data,
        original_data=original_data,
        masks=masks,
        input_ob_masks=input_ob_masks,
        config=vars(config)
    )

    print(f"âœ… æ’è¡¥æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶:")
    print(f"  - æ’è¡¥æ•°æ®: {imputed_data.shape}")
    print(f"  - åŸå§‹æ•°æ®: {original_data.shape}")
    print(f"  - æ©ç : {masks.shape}")
    print(f"  - è¾“å…¥è§‚æµ‹æ©ç : {input_ob_masks.shape}")

    return imputed_data, original_data, masks


def main():
    # æ¸…ç†å†…å­˜
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    config = TrainConfig()
    print("è®­ç»ƒé…ç½®ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰:")
    for arg, value in vars(config.args).items():
        print(f"  {arg}: {value}")

    # è®¾ç½® SwanLab
    swanlab_run = setup_swanlab(config)

    # ç¯å¢ƒé…ç½®
    base_dir = f"./tmp/imputation/{config.in_len}/{config.area}/STIMP-fixed/"
    check_dir(base_dir)
    seed_everything(1234)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ—¥å¿—é…ç½®
    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    log_path = os.path.join(base_dir, f'{timestamp}_missing_{config.missing_ratio}.log')
    logging.basicConfig(
        level=logging.INFO,
        filename=log_path,
        filemode='a',
        format='%(asctime)s - %(message)s',
        encoding='utf-8'
    )

    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    logging.getLogger().addHandler(console_handler)

    logging.info("STIMP Training Configuration (FIXED VERSION):")
    for arg, value in vars(config.args).items():
        logging.info(f"  {arg}: {value}")

    # è®­ç»ƒå‰æ•°æ®éªŒè¯
    if not validate_data_before_training(config):
        print("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
        return

    # åŠ è½½æ•°æ®é›†
    os.environ['RAW_DATA_PATH'] = config.raw_data_path
    train_dataset = PRE8dDataset(config.args, mode='train')
    val_dataset = PRE8dDataset(config.args, mode='val')
    test_dataset = PRE8dDataset(config.args, mode='test')

    # è®°å½•æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    data_stats = {
        "data/total_samples": len(train_dataset.all_samples),
        "data/train_samples": len(train_dataset.samples),
        "data/val_samples": len(val_dataset.samples),
        "data/test_samples": len(test_dataset.samples),
        "data/nodes": train_dataset.total_nodes,
    }

    if hasattr(train_dataset, 'missing_rate'):
        data_stats["data/missing_rate"] = train_dataset.missing_rate

    logging.info("ğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    for key, value in data_stats.items():
        logging.info(f"  {key}: {value}")

    if swanlab_run:
        swanlab.log(data_stats)

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0
    )

    # åŠ è½½é‚»æ¥çŸ©é˜µ
    if config.area in ["Himawari", "Bohai"]:
        adj_path = os.path.join(config.raw_data_path, "adj.npy")
    else:
        adj_path = f"./data/{config.area}/adj.npy"

    if not os.path.exists(adj_path):
        raise FileNotFoundError(f"é‚»æ¥çŸ©é˜µæ–‡ä»¶ä¸å­˜åœ¨: {adj_path}")

    adj = np.load(adj_path).astype(np.float32)
    adj = torch.from_numpy(adj).float().to(device)

    # éªŒè¯é‚»æ¥çŸ©é˜µå½¢çŠ¶
    expected_nodes = 4443
    if adj.shape[0] != expected_nodes or adj.shape[1] != expected_nodes:
        print(f"âš ï¸ è­¦å‘Š: é‚»æ¥çŸ©é˜µå½¢çŠ¶ {adj.shape} ä¸é¢„æœŸèŠ‚ç‚¹æ•° {expected_nodes} ä¸åŒ¹é…")

    print(f"âœ… åŠ è½½é‚»æ¥çŸ©é˜µ: {adj.shape}")

    # åŠ è½½ç»Ÿè®¡é‡
    low_bound = torch.from_numpy(train_dataset.mean).float().to(device)
    high_bound = torch.from_numpy(train_dataset.std).float().to(device)

    # åˆå§‹åŒ–æ¨¡å‹
    from model.graphdiffusion import IAP_base
    model = IAP_base(config.args, low_bound, high_bound).to(device)

    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.wd)
    p1 = int(0.75 * config.epochs)
    p2 = int(0.9 * config.epochs)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[p1, p2], gamma=0.1)

    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    # åˆå§‹åŒ–æ—©åœæ­¢
    early_stopping_path = os.path.join(base_dir, f"early_stopping_best_{config.missing_ratio}.pth")
    early_stopping = EarlyStopping(
        patience=config.early_stopping_patience,
        verbose=True,
        delta=0.001,
        path=early_stopping_path
    )

    print(f"ğŸ¯ æ—©åœæ­¢å·²å¯ç”¨ï¼Œè€å¿ƒå€¼: {config.early_stopping_patience}")

    # è®­ç»ƒå¾ªç¯
    best_mae = float('inf')
    best_mse = float('inf')
    train_pbar = tqdm(range(1, config.epochs + 1), desc="è®­ç»ƒè¿›åº¦")

    for epoch in train_pbar:
        model.train()
        loss_meter = AverageMeter()
        data_time_meter = AverageMeter()
        end = time.time()

        for step, (datas, data_ob_masks, data_gt_masks, labels, label_masks) in enumerate(train_loader):
            # æ•°æ®ç§»è‡³è®¾å¤‡
            datas = datas.float().to(device)
            data_ob_masks = data_ob_masks.float().to(device)
            data_gt_masks = data_gt_masks.float().to(device)
            adj_batch = adj.repeat(datas.shape[0], 1, 1).to(device)

            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast():
                loss = model.trainstep(datas, data_ob_masks, adj_batch, 1)

            # æ›´æ–°æŒ‡æ ‡
            loss_meter.update(loss.item(), datas.shape[0])
            data_time_meter.update(time.time() - end)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
            end = time.time()

        # å­¦ä¹ ç‡è°ƒåº¦
        current_lr = scheduler.get_last_lr()[0]
        scheduler.step()

        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        train_metrics = {
            "train/loss": loss_meter.avg,
            "train/data_time": data_time_meter.avg,
            "train/learning_rate": current_lr,
        }

        # æ—¥å¿—è¾“å‡º
        log_msg = f"Epoch {epoch} | è®­ç»ƒæŸå¤±: {loss_meter.avg:.4f} | è€—æ—¶: {data_time_meter.avg:.2f}s | LR: {current_lr:.6f}"
        print(log_msg)
        logging.info(log_msg)
        train_pbar.set_description(f"Epoch {epoch} | æŸå¤±: {loss_meter.avg:.4f}")

        # ä¸Šä¼ æŒ‡æ ‡åˆ° SwanLab
        if swanlab_run:
            swanlab.log(train_metrics, step=epoch)

        # éªŒè¯é›†å’Œæµ‹è¯•é›†è¯„ä¼°ï¼ˆç”¨äºæ—©åœæ­¢å’Œç›‘æ§ï¼‰
        if epoch % config.test_freq == 0 or epoch == config.epochs:
            model.eval()
            val_mae_list, val_mse_list, val_rmse_list = [], [], []
            test_mae_list, test_mse_list, test_rmse_list = [], [], []

            # 1. éªŒè¯é›†è¯„ä¼°ï¼ˆç”¨äºæ—©åœæ­¢ï¼‰
            with torch.no_grad():
                for step, (datas, data_ob_masks, data_gt_masks, labels, label_masks) in enumerate(val_loader):
                    if step >= 5:  # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡ä»¥åŠ å¿«é€Ÿåº¦
                        break

                    # æ•°æ®ç§»è‡³è®¾å¤‡
                    datas = datas.float().to(device)
                    data_ob_masks = data_ob_masks.float().to(device)
                    data_gt_masks = data_gt_masks.float().to(device)
                    adj_batch = adj.repeat(datas.shape[0], 1, 1).to(device)

                    # æ’è¡¥
                    imputed = model.impute(datas, data_gt_masks, adj_batch, config.num_samples)

                    # è®¡ç®—å¤šç§æŒ‡æ ‡
                    mae, mse, rmse = calculate_metrics_fixed(imputed, datas, data_gt_masks)
                    val_mae_list.append(mae)
                    val_mse_list.append(mse)
                    val_rmse_list.append(rmse)

            # 2. æµ‹è¯•é›†è¯„ä¼°ï¼ˆä»…ç”¨äºç›‘æ§ï¼Œä¸ç”¨äºæ—©åœæ­¢ï¼‰
            with torch.no_grad():
                for step, (datas, data_ob_masks, data_gt_masks, labels, label_masks) in enumerate(test_loader):
                    if step >= 5:  # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡ä»¥åŠ å¿«é€Ÿåº¦
                        break

                    # æ•°æ®ç§»è‡³è®¾å¤‡
                    datas = datas.float().to(device)
                    data_ob_masks = data_ob_masks.float().to(device)
                    data_gt_masks = data_gt_masks.float().to(device)
                    adj_batch = adj.repeat(datas.shape[0], 1, 1).to(device)

                    # æ’è¡¥
                    imputed = model.impute(datas, data_gt_masks, adj_batch, config.num_samples)

                    # è®¡ç®—å¤šç§æŒ‡æ ‡
                    mae, mse, rmse = calculate_metrics_fixed(imputed, datas, data_gt_masks)
                    test_mae_list.append(mae)
                    test_mse_list.append(mse)
                    test_rmse_list.append(rmse)

            # å¹³å‡éªŒè¯é›†æŒ‡æ ‡
            if val_mae_list:
                avg_val_mae = torch.stack(val_mae_list).mean().item()
                avg_val_mse = torch.stack(val_mse_list).mean().item()
                avg_val_rmse = torch.stack(val_rmse_list).mean().item()

                val_metrics = {
                    "val/mae": avg_val_mae,
                    "val/mse": avg_val_mse,
                    "val/rmse": avg_val_rmse,
                }

                val_msg = f"éªŒè¯ | MAE: {avg_val_mae:.4f} | MSE: {avg_val_mse:.4f} | RMSE: {avg_val_rmse:.4f}"
                print(val_msg)
                logging.info(val_msg)

            # å¹³å‡æµ‹è¯•é›†æŒ‡æ ‡
            if test_mae_list:
                avg_test_mae = torch.stack(test_mae_list).mean().item()
                avg_test_mse = torch.stack(test_mse_list).mean().item()
                avg_test_rmse = torch.stack(test_rmse_list).mean().item()

                test_metrics = {
                    "test/mae": avg_test_mae,
                    "test/mse": avg_test_mse,
                    "test/rmse": avg_test_rmse,
                }

                test_msg = f"æµ‹è¯• | MAE: {avg_test_mae:.4f} | MSE: {avg_test_mse:.4f} | RMSE: {avg_test_rmse:.4f}"
                print(test_msg)
                logging.info(test_msg)

            # ä¸Šä¼ æŒ‡æ ‡åˆ° SwanLab
            if swanlab_run:
                swanlab.log(val_metrics, step=epoch)
                swanlab.log(test_metrics, step=epoch)

            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†æŒ‡æ ‡ï¼‰
            if val_mae_list and avg_val_mae < best_mae:
                best_mae = avg_val_mae
                best_mse = avg_val_mse
                best_model_path = os.path.join(base_dir, f"best_{config.missing_ratio}.pth")
                torch.save(model.state_dict(), best_model_path)
                best_msg = f" ä¿å­˜æœ€ä½³æ¨¡å‹ | éªŒè¯MAE: {best_mae:.4f} | éªŒè¯MSE: {best_mse:.4f}"
                print(best_msg)
                logging.info(best_msg)

                if swanlab_run:
                    swanlab.log({"best/val_mae": best_mae, "best/val_mse": best_mse}, step=epoch)

            # æ—©åœæ­¢æ£€æŸ¥ - ä½¿ç”¨éªŒè¯é›†MAEä½œä¸ºæ—©åœæ­¢æŒ‡æ ‡
            if val_mae_list:
                early_stopping(avg_val_mae, model)

                if early_stopping.early_stop:
                    print(f"ğŸ›‘ æ—©åœæ­¢è§¦å‘ï¼åœ¨ epoch {epoch} åœæ­¢è®­ç»ƒ")
                    logging.info(f"æ—©åœæ­¢è§¦å‘äº epoch {epoch}")
                    break

    # è®­ç»ƒå®Œæˆ
    final_msg = f"è®­ç»ƒç»“æŸ | æœ€ä½³éªŒè¯MAE: {best_mae:.4f} | æœ€ä½³éªŒè¯MSE: {best_mse:.4f}"
    print(final_msg)
    logging.info(final_msg)

    # åŠ è½½æ—©åœæ­¢ä¿å­˜çš„æœ€ä½³æ¨¡å‹
    if os.path.exists(early_stopping_path):
        print(f"ğŸ“¥ åŠ è½½æ—©åœæ­¢ä¿å­˜çš„æœ€ä½³æ¨¡å‹: {early_stopping_path}")
        model.load_state_dict(torch.load(early_stopping_path))

    # æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°
    print("ğŸ§ª å¼€å§‹æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    model.eval()
    test_mae_list = []
    test_mse_list = []
    test_rmse_list = []

    with torch.no_grad():
        for step, (datas, data_ob_masks, data_gt_masks, labels, label_masks) in enumerate(test_loader):
            if step >= 10:
                break

            # æ•°æ®ç§»è‡³è®¾å¤‡
            datas = datas.float().to(device)
            data_ob_masks = data_ob_masks.float().to(device)
            data_gt_masks = data_gt_masks.float().to(device)
            adj_batch = adj.repeat(datas.shape[0], 1, 1).to(device)

            # æ’è¡¥
            imputed = model.impute(datas, data_gt_masks, adj_batch, config.num_samples)

            # è®¡ç®—å¤šç§æŒ‡æ ‡
            mae, mse, rmse = calculate_metrics_fixed(imputed, datas, data_gt_masks)
            test_mae_list.append(mae)
            test_mse_list.append(mse)
            test_rmse_list.append(rmse)

    # è®¡ç®—æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡
    if test_mae_list:
        final_test_mae = torch.stack(test_mae_list).mean().item()
        final_test_mse = torch.stack(test_mse_list).mean().item()
        final_test_rmse = torch.stack(test_rmse_list).mean().item()

        final_test_msg = f"æœ€ç»ˆæµ‹è¯• | MAE: {final_test_mae:.4f} | MSE: {final_test_mse:.4f} | RMSE: {final_test_rmse:.4f}"
        print(final_test_msg)
        logging.info(final_test_msg)

        if swanlab_run:
            swanlab.log({
                "final_test/mae": final_test_mae,
                "final_test/mse": final_test_mse,
                "final_test/rmse": final_test_rmse
            })

    # ç”Ÿæˆæ’è¡¥æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if config.generate_imputation:
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆæ’è¡¥æ•°æ®...")
        imputed_data, original_data, masks = generate_imputation_data(config.args, model, test_loader, adj, device)

        # è®°å½•ç”Ÿæˆç»“æœ
        if swanlab_run:
            swanlab.log({
                "generation/samples_generated": imputed_data.shape[0],
                "generation/timestamp": timestamp
            })

    if swanlab_run:
        swanlab.log({"final/best_mae": best_mae, "final/best_mse": best_mse})
        swanlab.finish()

    return best_mae, best_mse


if __name__ == '__main__':
    main()